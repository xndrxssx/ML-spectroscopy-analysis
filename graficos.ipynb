{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f55ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessários\n",
    "import os\n",
    "import pywt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import joblib\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.signal import savgol_filter, detrend\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# # Cria diretórios para organizar todas as imagens salvas\n",
    "# print(\"Criando diretórios para salvar os gráficos...\")\n",
    "# os.makedirs('graficos_outliers_pca', exist_ok=True)\n",
    "# os.makedirs('graficos_outliers_boxplot', exist_ok=True)\n",
    "# os.makedirs('modelos_salvos', exist_ok=True)\n",
    "# # A pasta para os modelos será criada depois, mas podemos garantir aqui\n",
    "# # Nome da pasta para o notebook geral\n",
    "# os.makedirs('graficos_modelos', exist_ok=True) \n",
    "# # Nome da pasta para o notebook MLPR\n",
    "# os.makedirs('graficos_mlpr', exist_ok=True) \n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações de plot\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59462ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados: 250 amostras, 2151 comprimentos de onda\n",
      "Faixa espectral: 350.0 - 2500.0 nm\n",
      "Atributos disponíveis: ['Wavelength', 'SST', 'PH', 'AT', 'FIRMEZA (N)', 'UBU (%)', 'UBS (%)']\n",
      "Atributos a analisar: ['AT', 'FIRMEZA (N)', 'PH', 'SST', 'UBS (%)']\n"
     ]
    }
   ],
   "source": [
    "# Função para carregar dados\n",
    "def load_data(filepath):\n",
    "    \"\"\"Carrega dados espectrais e separa metadados de espectros.\"\"\"\n",
    "    df = pd.read_excel(filepath, engine='openpyxl')\n",
    "    \n",
    "    # Identificar colunas que são comprimentos de onda (numéricas)\n",
    "    numeric_cols = []\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            float(col)\n",
    "            numeric_cols.append(col)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Separar metadados e comprimentos de onda\n",
    "    metadata = df.drop(columns=numeric_cols)\n",
    "    wavelengths = df[numeric_cols]\n",
    "    \n",
    "    return metadata, wavelengths\n",
    "\n",
    "# Carregar dados\n",
    "filepath = 'Data/Original/dataset_cotton_fruit.xlsx'\n",
    "metadata, wavelengths = load_data(filepath)\n",
    "X = wavelengths.values\n",
    "wavelength_values = wavelengths.columns.astype(float)\n",
    "atributos = ['AT', 'FIRMEZA (N)', 'PH', 'SST', 'UBS (%)']\n",
    "\n",
    "print(f'Dados carregados: {X.shape[0]} amostras, {X.shape[1]} comprimentos de onda')\n",
    "print(f'Faixa espectral: {wavelength_values.min():.1f} - {wavelength_values.max():.1f} nm')\n",
    "print(f'Atributos disponíveis: {list(metadata.columns)}')\n",
    "print(f'Atributos a analisar: {atributos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dec2fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtros independentes de y: ['Raw', 'MSC', 'SNV', 'SG_D1', 'SG_D2', 'Detrend', 'Normalize', 'EMSC', 'Continuum_Removal', 'Wavelet_Denoising']\n",
      "Filtros dependentes de y: ['OSC_1', 'OSC_2', 'MSC_SG_OSC', 'OPLS1_SNV_SG_D1', 'OPLS2_SNV_SG_D1', 'SNV_Detrend_SG_D1']\n",
      "Total de filtros: 16\n"
     ]
    }
   ],
   "source": [
    "# Implementação dos filtros de pré-processamento\n",
    "# Filtros independentes de y\n",
    "def msc(X):\n",
    "    \"\"\"Multiplicative Scatter Correction.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    mean_spectrum = np.mean(X, axis=0)\n",
    "    corrected_spectra = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        slope, intercept = np.polyfit(mean_spectrum, X[i, :], 1)\n",
    "        corrected_spectra[i, :] = (X[i, :] - intercept) / slope\n",
    "    return corrected_spectra\n",
    "\n",
    "def snv(X):\n",
    "    \"\"\"Standard Normal Variate.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    return (X - np.mean(X, axis=1, keepdims=True)) / np.std(X, axis=1, keepdims=True)\n",
    "\n",
    "def savitzky_golay(X, window_size=11, poly_order=2, deriv_order=1):\n",
    "    \"\"\"Savitzky-Golay filter.\"\"\"\n",
    "    return savgol_filter(X, window_length=window_size, polyorder=poly_order, deriv=deriv_order, axis=1)\n",
    "\n",
    "def detrend_filter(X):\n",
    "    \"\"\"Detrending filter.\"\"\"\n",
    "    return detrend(X, axis=1)\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\"Normalização Min-Max.\"\"\"\n",
    "    return (X - np.min(X, axis=1, keepdims=True)) / (np.max(X, axis=1, keepdims=True) - np.min(X, axis=1, keepdims=True))\n",
    "\n",
    "def emsc(X, reference=None):\n",
    "    \"\"\"Extended Multiplicative Signal Correction.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    if reference is None:\n",
    "        reference = np.mean(X, axis=0)  # Usa o espectro médio como referência\n",
    "    \n",
    "    X_corr = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        # Modelo: X[i] ≈ a + b*reference\n",
    "        model = np.vstack([np.ones_like(reference), reference]).T\n",
    "        params, _, _, _ = np.linalg.lstsq(model, X[i, :], rcond=None)\n",
    "        a, b = params[0], params[1]\n",
    "        X_corr[i,:] = (X[i, :] - a) / b\n",
    "    return X_corr\n",
    "\n",
    "def continuum_removal(X, wavelengths):\n",
    "    \"\"\"Continuum Removal.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    X_cr = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        spectrum = X[i, :]\n",
    "        # Encontra os pontos do casco convexo superior\n",
    "        q_u = [0]\n",
    "        for k in range(1, len(wavelengths) - 1):\n",
    "            s_k = (spectrum[len(wavelengths)-1] - spectrum[0]) / (wavelengths[-1] - wavelengths[0])\n",
    "            s_q = (spectrum[k] - spectrum[q_u[-1]]) / (wavelengths[k] - wavelengths[q_u[-1]])\n",
    "            if s_q > s_k:\n",
    "                q_u.append(k)\n",
    "        q_u.append(len(wavelengths)-1)\n",
    "        \n",
    "        # Interpolação linear entre os pontos do casco\n",
    "        continuum = np.interp(wavelengths, wavelengths[q_u], spectrum[q_u])\n",
    "        X_cr[i, :] = spectrum / continuum\n",
    "    return X_cr\n",
    "\n",
    "def wavelet_denoising(X, wavelet='db4', level=4):\n",
    "    \"\"\"Wavelet Transform para Denoising.\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    original_length = X.shape[1]\n",
    "    denoised_list = []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        # 1. Decomposição Wavelet\n",
    "        coeffs = pywt.wavedec(X[i, :], wavelet, level=level)\n",
    "\n",
    "        # 2. Cálculo do limiar (threshold)\n",
    "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
    "        threshold = sigma * np.sqrt(2 * np.log(original_length))\n",
    "\n",
    "        # 3. Aplicação do filtro (soft thresholding) nos coeficientes de detalhe\n",
    "        coeffs[1:] = [pywt.threshold(c, value=threshold, mode='soft') for c in coeffs[1:]]\n",
    "\n",
    "        # 4. Reconstrução do sinal\n",
    "        reconstructed_signal = pywt.waverec(coeffs, wavelet)\n",
    "\n",
    "        # 5. Ajuste do tamanho\n",
    "        denoised_list.append(reconstructed_signal[:original_length])\n",
    "\n",
    "    return np.asarray(denoised_list)\n",
    "\n",
    "# Filtros dependentes de y (Orthogonal Signal Correction)\n",
    "class OrthogonalCorrection:\n",
    "    \"\"\"Orthogonal Signal Correction (OSC).\"\"\"\n",
    "    def __init__(self, n_components=1):\n",
    "        self.n_components = n_components\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        X, y = np.asarray(X), np.asarray(y).ravel()\n",
    "        self.w_ortho_ = []\n",
    "        self.p_ortho_ = []\n",
    "        self.X_corr_ = X.copy()\n",
    "        \n",
    "        for _ in range(self.n_components):\n",
    "            pls = PLSRegression(n_components=1)\n",
    "            pls.fit(self.X_corr_, y)\n",
    "            t = pls.x_scores_\n",
    "            w = pls.x_weights_\n",
    "            p = pls.x_loadings_\n",
    "            \n",
    "            # Componente Ortogonal\n",
    "            w_ortho = p - (np.dot(w.T, p) / np.dot(w.T, w)) * w\n",
    "            t_ortho = np.dot(self.X_corr_, w_ortho)\n",
    "            p_ortho = np.dot(t_ortho.T, self.X_corr_) / np.dot(t_ortho.T, t_ortho)\n",
    "            \n",
    "            # Remover variação ortogonal\n",
    "            self.X_corr_ -= np.dot(t_ortho, p_ortho)\n",
    "            self.w_ortho_.append(w_ortho)\n",
    "            self.p_ortho_.append(p_ortho)\n",
    "        \n",
    "        return self.X_corr_\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_res = np.asarray(X).copy()\n",
    "        for i in range(self.n_components):\n",
    "            t_ortho = np.dot(X_res, self.w_ortho_[i])\n",
    "            X_res -= np.dot(t_ortho, self.p_ortho_[i])\n",
    "        return X_res\n",
    "\n",
    "# Dicionário de filtros independentes de y\n",
    "filtros_independentes = {\n",
    "    'Raw': lambda X: X,\n",
    "    'MSC': msc,\n",
    "    'SNV': snv,\n",
    "    'SG_D1': lambda X: savitzky_golay(X, window_size=11, poly_order=2, deriv_order=1),\n",
    "    'SG_D2': lambda X: savitzky_golay(X, window_size=11, poly_order=2, deriv_order=2),\n",
    "    'Detrend': detrend_filter,\n",
    "    'Normalize': normalize,\n",
    "    'EMSC': emsc,\n",
    "    'Continuum_Removal': lambda X: continuum_removal(X, wavelength_values),\n",
    "    'Wavelet_Denoising': wavelet_denoising\n",
    "}\n",
    "\n",
    "# Dicionário de filtros dependentes de y\n",
    "filtros_dependentes = {\n",
    "    'OSC_1': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(X, y),\n",
    "    'OSC_2': lambda X, y: OrthogonalCorrection(n_components=2).fit_transform(X, y),\n",
    "    'MSC_SG_OSC': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(\n",
    "        savitzky_golay(msc(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
    "    'OPLS1_SNV_SG_D1': lambda X, y: OrthogonalCorrection(n_components=1).fit_transform(\n",
    "        savitzky_golay(snv(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
    "    'OPLS2_SNV_SG_D1': lambda X, y: OrthogonalCorrection(n_components=2).fit_transform(\n",
    "        savitzky_golay(snv(X), window_size=11, poly_order=2, deriv_order=1), y),\n",
    "    'SNV_Detrend_SG_D1': lambda X, y: savitzky_golay(detrend_filter(snv(X)), window_size=11, poly_order=2, deriv_order=1)\n",
    "}\n",
    "\n",
    "print(f'Filtros independentes de y: {list(filtros_independentes.keys())}')\n",
    "print(f'Filtros dependentes de y: {list(filtros_dependentes.keys())}')\n",
    "print(f'Total de filtros: {len(filtros_independentes) + len(filtros_dependentes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29391968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_filtros_independentes(X, wavelength_values, filtros_independentes):\n",
    "    \"\"\"\n",
    "    Plota os gráficos dos filtros independentes aplicados ao dataset.\n",
    "\n",
    "    Parâmetros:\n",
    "    - X: np.ndarray, dados espectrais originais (amostras x comprimentos de onda).\n",
    "    - wavelengths_values: np.ndarray, valores dos comprimentos de onda.\n",
    "    - filtros_independentes: dict, dicionário de funções de filtros independentes.\n",
    "\n",
    "    Retorno:\n",
    "    - Plots dos espectros processados por cada filtro.\n",
    "    \"\"\"\n",
    "    num_filtros = len(filtros_independentes)\n",
    "    plt.figure(figsize=(15, 5 * num_filtros))\n",
    "\n",
    "    for i, (nome_filtro, func_filtro) in enumerate(filtros_independentes.items()):\n",
    "        # Aplica o filtro\n",
    "        X_filtrado = func_filtro(X)\n",
    "\n",
    "        # Plota o espectro filtrado\n",
    "        plt.subplot(num_filtros, 1, i + 1)\n",
    "        for espectro in X_filtrado:\n",
    "            plt.plot(wavelength_values, espectro, alpha=0.7)\n",
    "        plt.title(f\"Filtro: {nome_filtro}\", fontsize=14)\n",
    "        plt.xlabel(\"Comprimento de Onda (nm)\", fontsize=12)\n",
    "        plt.ylabel(\"Intensidade\", fontsize=12)\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be5ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
